{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c3d6ef-e81c-4361-83a5-d998d0d722fd",
   "metadata": {},
   "source": [
    "## ED22B052 Lab11_A Neural Networks Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0886b3ee-876e-49eb-b50e-9895b5f3c1dc",
   "metadata": {},
   "source": [
    "#### Problem Statement: Implement the forward propagation for a two-hidden-layer network for m-samples and n-features, as we discussed in class. Initialize the weights randomly. Use the data from the previous labs, such as logistic regression. You can choose the number of neurons in the hidden layer and use the sigmoid activation function. Report the evaluation metrics for the network.  Also, use other non-linear activation functions like ReLU and Tanh. Report the loss using both MSE and Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "921b5eaf-e69c-4e71-a8a5-d14adba199d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Evaluation:\n",
      "========================================\n",
      "Using: Sigmoid Activation\n",
      "Mean Squared Error: 3.6749\n",
      "Cross Entropy: 0.6937\n",
      "----------------------------------------\n",
      "Using: ReLU Activation\n",
      "Mean Squared Error: 3.6940\n",
      "Cross Entropy: 0.6931\n",
      "----------------------------------------\n",
      "Using: Tanh Activation\n",
      "Mean Squared Error: 3.6940\n",
      "Cross Entropy: 0.6931\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('3D_printing_data.csv')\n",
    "\n",
    "# Convert text labels into numbers\n",
    "material_encoder = LabelEncoder()\n",
    "pattern_encoder = LabelEncoder()\n",
    "data['material'] = material_encoder.fit_transform(data['material'])\n",
    "data['infill_pattern'] = pattern_encoder.fit_transform(data['infill_pattern'])\n",
    "\n",
    "# Separate input features and the output we want to predict\n",
    "features = data.drop(columns=['elongation'])  # Predicting 'elongation'\n",
    "target = data['elongation'].values.reshape(-1, 1)\n",
    "\n",
    "# Make all feature values similar in scale\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Simple Neural Network class\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden1_nodes, hidden2_nodes, output_nodes, activation):\n",
    "        np.random.seed(42)\n",
    "        self.weights1 = np.random.randn(input_nodes, hidden1_nodes) * 0.01\n",
    "        self.bias1 = np.zeros((1, hidden1_nodes))\n",
    "        self.weights2 = np.random.randn(hidden1_nodes, hidden2_nodes) * 0.01\n",
    "        self.bias2 = np.zeros((1, hidden2_nodes))\n",
    "        self.weights3 = np.random.randn(hidden2_nodes, output_nodes) * 0.01\n",
    "        self.bias3 = np.zeros((1, output_nodes))\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward_pass(self, inputs):\n",
    "        self.layer1 = np.dot(inputs, self.weights1) + self.bias1\n",
    "        self.activation1 = self.activation(self.layer1)\n",
    "        \n",
    "        self.layer2 = np.dot(self.activation1, self.weights2) + self.bias2\n",
    "        self.activation2 = self.activation(self.layer2)\n",
    "        \n",
    "        self.output_layer = np.dot(self.activation2, self.weights3) + self.bias3\n",
    "        return self.output_layer\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.forward_pass(inputs)\n",
    "\n",
    "# Check how good the model is\n",
    "def check_model_performance(model, inputs, real_values):\n",
    "    predictions = model.predict(inputs)\n",
    "    \n",
    "    mse_score = mean_squared_error(real_values, predictions)\n",
    "    \n",
    "    # For binary classification version (used in log loss)\n",
    "    binary_real = (real_values > np.median(real_values)).astype(int)\n",
    "    prediction_probs = sigmoid(predictions)\n",
    "    \n",
    "    log_loss_score = log_loss(binary_real, prediction_probs)\n",
    "    \n",
    "    return mse_score, log_loss_score\n",
    "\n",
    "# Set up the network\n",
    "input_nodes = x_train.shape[1]\n",
    "hidden1_nodes = 10\n",
    "hidden2_nodes = 5\n",
    "output_nodes = 1\n",
    "\n",
    "# Try different activation functions\n",
    "networks = {\n",
    "    'Sigmoid': SimpleNeuralNetwork(input_nodes, hidden1_nodes, hidden2_nodes, output_nodes, sigmoid),\n",
    "    'ReLU': SimpleNeuralNetwork(input_nodes, hidden1_nodes, hidden2_nodes, output_nodes, relu),\n",
    "    'Tanh': SimpleNeuralNetwork(input_nodes, hidden1_nodes, hidden2_nodes, output_nodes, tanh)\n",
    "}\n",
    "\n",
    "# Test each network\n",
    "results = {}\n",
    "for name, network in networks.items():\n",
    "    network.forward_pass(x_train)\n",
    "    mse_score, log_loss_score = check_model_performance(network, x_test, y_test)\n",
    "    results[name] = {'MSE': mse_score, 'Cross Entropy': log_loss_score}\n",
    "\n",
    "# Show the results\n",
    "print(\"Neural Network Evaluation:\")\n",
    "print(\"=\" * 40)\n",
    "for name, scores in results.items():\n",
    "    print(f\"Using: {name} Activation\")\n",
    "    print(f\"Mean Squared Error: {scores['MSE']:.4f}\")\n",
    "    print(f\"Cross Entropy: {scores['Cross Entropy']:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4696f-a5c4-425f-a934-830d727ee765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
